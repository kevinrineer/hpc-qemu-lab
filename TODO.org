#+TITLE: QEMU Lab TODOs
#+AUTHOR: Kevin Rineer
#+OPTIONS: toc:t num:nil
* TODO Virtual HPC Lab Project Tasks (Network-First Workflow)
** DONE Setup and Prep
   CLOSED: [2025-06-17 Tue 08:25]
   - [X] Ensure host system has required software (Git, QEMU, Ansible) - check_reqs.sh
   - [X] Download Rocky Linux qcow2 cloud image for VM base - download_qcow2.sh
** Windows Head Node Setup test
        Tool: Hyper-V (Recommended) VirtualBox (Backup).
        Warewulf Head Node VM: Create one Linux VM (e.g., Rocky Linux) in Hyper-V Manager.
            Network Adapter 1 (Management): Connect to the "Default Switch" (or an "External" switch for direct bridging) for host access (SSH) and internet.
            Network Adapter 2 (Warewulf Private): First, create a new "Internal" Virtual Switch in Hyper-V Manager (e.g., "Warewulf Lab Network"). Ensure "Allow management operating system to share this network adapter" is checked. Hyper-V does not provide DHCP on Internal or Private switches by default, which is ideal. Then, connect the second adapter of the head node VM to this "Warewulf Lab Network" switch.
        Warewulf Compute Node VMs: Create multiple Linux VMs in Hyper-V Manager.
            Network Adapter: Connect only their single network adapter to the same "Warewulf Lab Network" (Internal Virtual Switch) as the Warewulf head node's second adapter.
            Boot Order: Configure these VMs for PXE boot.
        (Alternative Tool): VirtualBox is also a viable option if Hyper-V is not preferred or available. The setup would involve creating a "Host-only Network" and ensuring its DHCP server is disabled.
** MacOS Head Node Setup test
        Tool: UTM. (This handles the QEMU/HVF acceleration seamlessly).
        Warewulf Head Node VM: Create one Linux VM (e.g., Rocky Linux) in UTM.
            Network Adapter 1 (Management): Set to "Shared Network" (NAT) in UTM for host access (SSH) and internet connectivity.
            Network Adapter 2 (Warewulf Private): Create a new "Virtual Network" in UTM (this acts as a host-only/internal network). Crucially, ensure DHCP is disabled for this virtual network in UTM's settings. Connect the second adapter of the head node VM to this network.
        Warewulf Compute Node VMs: Create multiple Linux VMs in UTM.
            Network Adapter: Connect only their single network adapter to the same "Virtual Network" as the Warewulf head node's second adapter.
            Boot Order: Configure these VMs for PXE boot.
** 172.16.0.0/24 Virtual Networking Setup (Foundation)
   - [ ] Create virtual network bridge or tap interface on host (e.g., br0)
   - [ ] Assign static IP 172.16.0.1 to host bridge as gateway
   - [ ] Configure IP forwarding and firewall rules for isolation and SSH forwarding
   - [ ] Write `setup_network.sh` script to automate bridge creation, IP assignment, tap device setup
   - [ ] Write `teardown_network.sh` script to cleanly dismantle networking environment

** VM Infrastructure Setup (Build on Network)
   - [ ] Prepare image customization (cloud-init or manual config) for initial setup - 
   - [ ] shell scripts launch Central IT VM with network interface on created bridge and static IP
   - [ ] shell scripts launch Storage Node VM similarly with static IP
   - [ ] shell scripts launch Head Node VM similarly with static IP
   - [ ] shell scripts launch Login Node VM similarly with static IP
   - [ ] Launch minimal VMs (Central IT and Head Node) to validate network and VM operation
   - [ ] Expand scripts to launch Login Node VM
   - [ ] Expand scripts to launch Compute Node VMs (support multiple instances)
   - [ ] Implement `start`, `stop`, and `cleanup` commands for whole lab environment in lab.sh

** Foundational step validation
   - [ ] Test VM-to-VM connectivity with basic ping and SSH using placeholder or minimal VMs
   - [ ] Document subnet and IP assignments in README/design notes

** Provisioning (Configure Software Stack)
   - [ ] Create Ansible inventory covering all VMs with their static IPs
   - [ ] Develop Ansible playbooks/roles:
       - Central IT: OpenLDAP server setup
       - Storage Node: NFS server
       - Head Node: Warewulf, Slurm controller
       - Login Node: LDAP client, Slurm worker, NFS mount
   - [ ] Integrate Apptainer container runtime on relevant nodes
   - [ ] Test user authentication via LDAP from all nodes

** Testing and Validation
   - [ ] Verify PXE boot process on compute and login nodes
   - [ ] Test Slurm job scheduling and execution
   - [ ] Validate NFS mounts and file sharing
   - [ ] Confirm containerized workload execution with Apptainer
   - [ ] Verify SSH access and port forwarding works as intended

** Documentation
   - [ ] Update README with step-by-step instructions and architecture overview
   - [ ] Document network design and IP scheme
   - [ ] Include sample job scripts and usage examples
   - [ ] Anything else?

** Optional Enhancements
   - [ ] Support Vagrant provisioning?
   - [ ] Make the Login node provisioned by warewulf instead of ansible
   - [ ] Support additional Linux distros?
