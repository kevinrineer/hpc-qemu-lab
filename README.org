#+TITLE: Virtual HPC Lab with QEMU, Warewulf, Slurm, Apptainer, LDAP, and NFS
#+AUTHOR: Kevin Rineer
#+OPTIONS: toc:t num:nil
#+bibliography: references.bib
#+cite_export: biblatex

* Overview
This project provides a self-contained, cross-platform virtual HPC lab environment using QEMU and shell scripts to simulate a small compute cluster. With inspiration from [cite:@hpc-toolset-tutorial] this lab setup contains a stack of software components to simulate a small cluster in which new administrators can gain skills.

The lab includes:
- A manually provisioned head node (for control and management of the cluster)
- A manually provisioned "Central IT" VM running unencrypted OpenLDAP as an external identity provider (LDAPS and KRB5 might be used for encryption in production)
- A login node and multiple compute nodes provisioned by the head node via Warewulf
- Integration with:
  - Slurm (job scheduler)
  - Apptainer (containerized workloads)
  - OpenLDAP (central user authentication)
  - NFS (shared /home filesystem)

The environment is ideal for students, researchers, or sysadmins who want to learn or test HPC management tools and configuration using simpler hardware than what might be used in a traditional cluster.

The rationale for why this exists when hpc-toolset-tutorial already does is that this lab provides a different, opinionated, software stack. This is a pretty minimal set of software and lacks the feature set that the hpc-toolset-tutorial's components provides, which I claim to be an advantage.

* Key Features
- Cross-platform (Linux, macOS, Windows via WSL2)
- Fully automated VM control via shell scripts
- Ansible-driven provisioning of cluster software inside VMs not provisioned by Warewulf
- Internal virtual network with SSH port forwarding
- Supports PXE boot and image deployment with Warewulf

* Requirements
- Git installed on host
- QEMU installed on host
- shell environment (e.g. bash)
- Ansible (for provisioning tasks)
- Enough host resources (8+ GB RAM, 40+ GB disk, 4+ CPU cores recommended)
- This lab presumes that Rocky Linux is the target Linux distribution for the cluster

* Project Structure
hpc-qemu-lab/
├── ansible/                     # Ansible provisioning roles and inventory to configure the "Central IT" and head nodes
├── docs/                        # Lab walkthroughs and learning materials (coming eventually)
├── images/                      # QEMU disk images (.qcow2) created at runtime
├── iso/                         # Rocky Linux ISO download directory
├── logs/                        # Optional: logs or serial output from VMs
├── lib/                         # Shared shell utilities and constants
├── scripts/                     # Core VM and network management scripts
├── config.sh                    # Global config: VM specs, version, network
├── lab.sh                       # Main CLI wrapper script (start/stop/status/ssh/cleanup)
├── README.org                   # This file
├── LICENSE
└── references.bib               # for Org-mode citation support

* Usage

** Obtain the lab via git

   #+BEGIN_SRC shell
git clone https://github.com/kevinrineer/hpc-qemu-lab.git
cd hpc-qemu-lab
   #+END_SRC

** Start the Lab
#+BEGIN_SRC shell
./scripts/lab.sh start
#+END_SRC

This will:
- Launch a head node and several compute/login nodes
- Set up virtual networking
- Enable SSH access to the head node (e.g., port 2222 → guest port 22)

** Provision the Cluster
Once VMs are up and reachable:

#+BEGIN_SRC shell
ansible-playbook -i ansible/inventory.yml ansible/site.yml
#+END_SRC

This configures:
- Head node services: Warewulf, Slurm controller, LDAP server, NFS server
- Login/compute nodes: LDAP clients, Slurm workers, NFS mounts

** Stop the Lab
#+BEGIN_SRC shell
./scripts/lab.sh stop
#+END_SRC

** Clean the Lab (destroy images, teardown virtual network, remove state)
#+BEGIN_SRC shell
./scripts/lab.sh cleanup
#+END_SRC

* Design Notes

- The head runs core cluster services, where the central-it node provides IAM via OpenLDAP.
- LDAP centralizes networked user accounts across all nodes.
- Login and compute nodes are provisioned *by the head node* using Warewulf and PXE boot.
- NFS provides a shared `/home` for user data and container storage.
- `/home-local` for users local to the specific VMs, for the potential emulation of loss and recovery of the LDAP or NFS utilities

* License
This project is licensed under the GPLv3 License. See =LICENSE= for details.

* Acknowledgments
This project is inspired by [cite:@hpc-toolset-tutorial], created by the Center for Computational Research at the University at Buffalo and aims to build on its concepts with a virtualized, portable lab setup.
The template for the README generated by ChatGPT.

* TODOs
See the file =TODO.org= for an up-to-date task list.

